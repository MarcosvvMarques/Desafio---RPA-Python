# Desafio---RPA-Python 

Este projeto consiste em realizar web scraping em um site para coletar citaÃ§Ãµes, seus autores e as tags associadas a cada um. O script utiliza as bibliotecas Pandas, Selenium e CSV para navegar, coletar dados e analisÃ¡-los. Por fim, foi automatizado o envio dos dados extraÃ­dos por e-mail atrÃ¡ves da biblioetca smtplib. 

## PrÃ©-requisitos: 

ğŸ”¹ Antes de rodar o projeto, instale as dependÃªncias necessÃ¡rias: 
ğŸ”¹ pip install -r requirements.txt 

- O arquivo requirementes.txt deve conter as seguintes bibliotecas: Selenium, pandas e python-dotenv

## ConfiguraÃ§Ã£o do E-mail:
ğŸ”¹As credenciais de e-mail e os destinatÃ¡rios do e-mail sÃ£o armazenados em um arquivo .env, que  tem o seguinte formato:

EMAIL_USER=seu_email@gmail.com
EMAIL_PASSWORD=sua_senha_de_email
EMAIL_RECIPIENTS=email1@example.com,email2@example.com


## InstruÃ§Ãµes de execuÃ§Ã£o:

1 - Crie o arquivo .env com as suas credenciais de e-mail e instale as dependÃªncias com pip install -r requirements.txt

2 - Execute o arquivo main.py para iniciar o processo de scraping e envio do e-mail: python ./app/main.py 

- ApÃ³s a execuÃ§Ã£o serÃ¡ possÃ­vel: 

ğŸ”¹Realizar o scraping das citaÃ§Ãµes;
ğŸ”¹Salvar as citaÃ§Ãµes em quotes.csv;
ğŸ”¹Calcular o nÃºmero de citaÃ§Ãµes, autor mais recorrente e tag mais recorrente;
ğŸ”¹Enviar um e-mail com o resumo do relatÃ³rio e o arquivo quotes.csv anexado.


## Funcionalidades:

**Web Scraping**: Coleta citaÃ§Ãµes de um site especÃ­fico (https://quotes.toscrape.com/js-delayed/), processa as informaÃ§Ãµes (texto, autor e tags), e as armazena em um arquivo CSV.

**Envio de E-mail**: Envia um e-mail com o relatÃ³rio gerado, incluindo um resumo das citaÃ§Ãµes, e anexa o arquivo CSV com as citaÃ§Ãµes extraÃ­das.

**AnÃ¡lise de Dados**: Calcula o nÃºmero total de citaÃ§Ãµes, identifica o autor mais recorrente e a tag mais recorrente.

## ExplicaÃ§Ã£o do CÃ³digo

**scraping_service.py**
- Esse mÃ³dulo contÃ©m as funÃ§Ãµes relacionadas ao processo de web scraping:

ğŸ”¹processar_lista_de_citacoes: Coleta todas as citaÃ§Ãµes de uma pÃ¡gina;
ğŸ”¹processar_citacao: Extrai os dados de uma citaÃ§Ã£o (texto, autor, tags);
ğŸ”¹obter_tags: ObtÃ©m as tags associadas a uma citaÃ§Ã£o;
ğŸ”¹processar_paginas: Navega entre as pÃ¡ginas de citaÃ§Ãµes e coleta todas as citaÃ§Ãµes.

**file_service.py**
- Esse mÃ³dulo lida com as operaÃ§Ãµes de leitura e escrita no arquivo CSV:

ğŸ”¹criar_arquivo_citacoes: Cria um arquivo CSV com as citaÃ§Ãµes extraÃ­das;
ğŸ”¹ler_citacoes_do_arquivo: LÃª o arquivo CSV e retorna uma lista com as citaÃ§Ãµes.

**email_service.py**
ğŸ”¹Esse mÃ³dulo Ã© responsÃ¡vel pelo envio de e-mails com o arquivo de citaÃ§Ãµes como anexo (CSV)

**data_service.py**
ğŸ”¹Esse mÃ³dulo ContÃ©m funÃ§Ãµes auxiliares, como contagem de citaÃ§Ãµes, identificaÃ§Ã£o do autor mais recorrente e a tag mais recorrente:

contar_citacoes: Conta o nÃºmero total de citaÃ§Ãµes.
obter_autor_mais_recorrente: Identifica o autor mais recorrente.
obter_tag_mais_recorrente: Identifica a tag mais recorrente.


## Estrutura do Projeto 

ğŸ”¹ Projeto RPA - Pasta principal do projeto onde estÃ£o separados por arquivos todas as funÃ§Ãµes, arquivo .env, .env.example e requirements.txt;

ğŸ”¹.env - Arquivo dos dados sensÃ­veis (credenciais de e-mail e senhas);

ğŸ”¹main.py - Arquivo principal para realizar a execuÃ§Ã£o do script;

ğŸ”¹scraping_service.py - MÃ³dulo para web scraping e extraÃ§Ã£o de dados;

ğŸ”¹file_service.py - MÃ³dulo para manipulaÃ§Ã£o de arquivos (CSV);

ğŸ”¹email_service.py - MÃ³dulo para enviar os e-mails;

ğŸ”¹data_service.py - FunÃ§Ãµes auxiliares para o projeto;

ğŸ”¹quotes.csv - Arquivo gerado com as citaÃ§Ãµes extraÃ­das;

ğŸ”¹README.md - este arquivo de documentaÃ§Ã£o.










